# Redes generativas

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

Las redes neuronales recurrentes (RNN) y sus variantes de c√©lulas cerradas, como las c√©lulas de memoria a corto plazo (LSTM) y las unidades recurrentes cerradas (GRU), proporcionaron un mecanismo para el modelado del lenguaje en el sentido de que pueden aprender el orden de las palabras y proporcionar predicciones para la siguiente palabra en un secuencia. Esto nos permite utilizar RNN para **tareas generativas**, como la generaci√≥n de texto normal, la traducci√≥n autom√°tica e incluso los subt√≠tulos de im√°genes.

> ‚úÖ Piensa en todas las veces que te has beneficiado de tareas generativas como completar texto mientras escribes. Investigue un poco sobre sus aplicaciones favoritas para ver si aprovecharon los RNN.

En la arquitectura RNN que analizamos en la unidad anterior, cada unidad RNN produjo el siguiente estado oculto como salida. Sin embargo, tambi√©n podemos agregar otra salida a cada unidad recurrente, lo que nos permitir√≠a generar una **secuencia** (que tiene la misma longitud que la secuencia original). Adem√°s, podemos usar unidades RNN que no aceptan una entrada en cada paso, y simplemente toman alg√∫n vector de estado inicial y luego producen una secuencia de salidas.

Esto permite diferentes arquitecturas neuronales que se muestran en la siguiente imagen:

![Image showing common recurrent neural network patterns.](images/unreasonable-effectiveness-of-rnn.jpg)

> Imagen de la publicaci√≥n del blog [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by [Andrej Karpaty](http://karpathy.github.io/)
>
> * **Uno a uno** es una red neuronal tradicional con una entrada y una salida
* **Uno a muchos** es una arquitectura generativa que acepta un valor de entrada y genera una secuencia de valores de salida. Por ejemplo, si queremos entrenar una red de **subt√≠tulos de im√°genes** que produzca una descripci√≥n textual de una imagen, podemos usar una imagen como entrada, pasarla a trav√©s de una CNN para obtener su estado oculto y luego tener una cadena recurrente. generar subt√≠tulos palabra por palabra
* **Muchos a uno** corresponde a las arquitecturas RNN que describimos en la unidad anterior, como la clasificaci√≥n de texto.
* **Muchos a muchos**, o **secuencia a secuencia** corresponde a tareas como **traducci√≥n autom√°tica**, donde primero hacemos que RNN recopile toda la informaci√≥n de la secuencia de entrada al estado oculto. y otra cadena RNN desenrolla este estado en la secuencia de salida.

En esta unidad, nos centraremos en modelos generativos simples que nos ayudan a generar texto. Para simplificar, utilizaremos la tokenizaci√≥n a nivel de personaje.

Entrenaremos a este RNN para generar texto paso a paso. En cada paso, tomaremos una secuencia de caracteres de longitud `nchars` y le pediremos a la red que genere el siguiente car√°cter de salida para cada car√°cter de entrada:

![Image showing an example RNN generation of the word 'HELLO'.](images/rnn-generate.png)

Al generar texto (durante la inferencia), comenzamos con alg√∫n **mensaje**, que se pasa a trav√©s de las celdas RNN para generar su estado intermedio, y luego desde este estado comienza la generaci√≥n. Generamos un car√°cter a la vez y pasamos el estado y el car√°cter generado a otra celda RNN para generar la siguiente, hasta que generemos suficientes caracteres.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Image by the author

## ‚úçÔ∏è Ejercicios: Redes Generativas

Contin√∫a tu aprendizaje en los siguientes cuadernos:

* [Generative Networks with PyTorch](GenerativePyTorch.ipynb)
* [Generative Networks with TensorFlow](GenerativeTF.ipynb)

## Generaci√≥n de texto suave y temperatura

La salida de cada celda RNN es una distribuci√≥n de probabilidad de caracteres. Si siempre tomamos el car√°cter con mayor probabilidad como el siguiente car√°cter en el texto generado, el texto a menudo puede "ciclarse" entre las mismas secuencias de caracteres una y otra vez, como en este ejemplo:

```
hoy del segundo la empresa y un segundo la empresa‚Ä¶
```

Sin embargo, si observamos la distribuci√≥n de probabilidad del siguiente car√°cter, podr√≠a ser que la diferencia entre algunas probabilidades m√°s altas no sea enorme, p. un car√°cter puede tener una probabilidad de 0,2, otro - 0,19, etc. Por ejemplo, cuando se busca el siguiente car√°cter en la secuencia '*play*', el siguiente car√°cter puede ser igualmente un espacio o **e** (como en el palabra *jugador*).

Esto nos lleva a la conclusi√≥n de que no siempre es "justo" seleccionar el personaje con mayor probabilidad, porque elegir el segundo m√°s alto a√∫n podr√≠a llevarnos a un texto significativo. Es m√°s prudente **muestrear** caracteres de la distribuci√≥n de probabilidad proporcionada por la salida de la red. Tambi√©n podemos usar un par√°metro, **temperatura**, que aplanar√° la distribuci√≥n de probabilidad, en caso de que queramos agregar m√°s aleatoriedad, o hacerla m√°s pronunciada, si queremos ce√±irnos m√°s a los caracteres de mayor probabilidad.

Explore c√≥mo se implementa esta generaci√≥n de texto suave en los cuadernos vinculados anteriormente.

## Conclusi√≥n

Si bien la generaci√≥n de texto puede ser √∫til por s√≠ sola, los principales beneficios provienen de la capacidad de generar texto utilizando RNN a partir de alg√∫n vector de caracter√≠sticas inicial. Por ejemplo, la generaci√≥n de texto se utiliza como parte de la traducci√≥n autom√°tica (secuencia a secuencia, en este caso el vector de estado del *codificador* se utiliza para generar o *decodificar* el mensaje traducido), o generar una descripci√≥n textual de una imagen (en la que caso, el vector de caracter√≠sticas provendr√≠a del extractor de CNN).

## üöÄ Desaf√≠o

Tome algunas lecciones en Microsoft Learn sobre este tema

* Generaci√≥n de texto con [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## Revisi√≥n y autoestudio

Aqu√≠ te dejamos algunos art√≠culos para ampliar tus conocimientos.

* Diferentes enfoques para la generaci√≥n de texto con Markov Chain, LSTM y GPT-2: [blog post](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Text generation sample in [Keras documentation](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Assignment](lab/README.md)

Hemos visto c√≥mo generar texto car√°cter por car√°cter. En la pr√°ctica de laboratorio, explorar√° la generaci√≥n de texto a nivel de palabra.

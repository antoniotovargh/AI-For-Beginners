# Redes neuronales recurrentes

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

En secciones anteriores, hemos estado utilizando representaciones sem√°nticas ricas de texto y un clasificador lineal simple adem√°s de las incrustaciones. Lo que hace esta arquitectura es capturar el significado agregado de las palabras en una oraci√≥n, pero no tiene en cuenta el **orden** de las palabras, porque la operaci√≥n de agregaci√≥n adem√°s de las incrustaciones elimin√≥ esta informaci√≥n del texto original. Debido a que estos modelos no pueden modelar el orden de las palabras, no pueden resolver tareas m√°s complejas o ambiguas, como la generaci√≥n de texto o la respuesta a preguntas.

Para capturar el significado de la secuencia de texto, necesitamos utilizar otra arquitectura de red neuronal, que se denomina **red neuronal recurrente** o RNN. En RNN, pasamos nuestra oraci√≥n a trav√©s de la red, un s√≠mbolo a la vez, y la red produce alg√∫n **estado**, que luego pasamos a la red nuevamente con el siguiente s√≠mbolo.

![RNN](./images/rnn.png)

> Imagen del autor

Dada la secuencia de entrada de tokens X<sub>0</sub>,...,X<sub>n</sub>, RNN crea una secuencia de bloques de red neuronal y entrena esta secuencia de un extremo a otro mediante retropropagaci√≥n. . Cada bloque de red toma un par (X<sub>i</sub>,S<sub>i</sub>) como entrada y produce S<sub>i+1</sub> como resultado. El estado final S<sub>n</sub> o (salida Y<sub>n</sub>) entra en un clasificador lineal para producir el resultado. Todos los bloques de red comparten los mismos pesos y se entrenan de un extremo a otro mediante un paso de retropropagaci√≥n.

Debido a que los vectores de estado S<sub>0</sub>,...,S<sub>n</sub> pasan a trav√©s de la red, es posible aprender las dependencias secuenciales entre palabras. Por ejemplo, cuando la palabra *no* aparece en alg√∫n lugar de la secuencia, puede aprender a negar ciertos elementos dentro del vector de estado, lo que resulta en negaci√≥n.

> ‚úÖ Dado que los pesos de todos los bloques RNN en la imagen de arriba son compartidos, la misma imagen se puede representar como un bloque (a la derecha) con un bucle de retroalimentaci√≥n recurrente, que devuelve el estado de salida de la red a la entrada.

## Anatom√≠a de una c√©lula RNN

Veamos c√≥mo se organiza una celda RNN simple. Acepta el estado anterior S<sub>i-1</sub> y el s√≠mbolo actual X<sub>i</sub> como entradas, y tiene que producir el estado de salida S<sub>i</sub> (y, A veces, tambi√©n estamos interesados en alg√∫n otro resultado Y<sub>i</sub>,como en el caso de las redes generativas).

Una celda RNN simple tiene dos matrices de peso en su interior: una transforma un s√≠mbolo de entrada (llam√©moslo W) y otra transforma un estado de entrada (H). En este caso, la salida de la red se calcula como &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), donde &sigma; es la funci√≥n de activaci√≥n y b es el sesgo adicional.

<img alt="RNN Cell Anatomy" src="images/rnn-anatomy.png" width="50%"/>

> Imagen del autor

En muchos casos, los tokens de entrada pasan a trav√©s de la capa de incrustaci√≥n antes de ingresar al RNN para reducir la dimensionalidad. En este caso, si la dimensi√≥n de los vectores de entrada es *emb_size* y el vector de estado es *hid_size*, el tama√±o de W es *emb_size*&times;*hid_size* y el tama√±o de H es *hid_size*&times;* tama√±o_hid*.

## Memoria a largo plazo (LSTM)

Uno de los principales problemas de los RNN cl√°sicos es el llamado problema de los **gradientes de fuga**. Debido a que los RNN se entrenan de un extremo a otro en un paso de retropropagaci√≥n, tienen dificultades para propagar el error a las primeras capas de la red y, por lo tanto, la red no puede aprender las relaciones entre tokens distantes. Una de las formas de evitar este problema es introducir **administraci√≥n de estado expl√≠cita** mediante el uso de las llamadas **puertas**. Hay dos arquitecturas bien conocidas de este tipo: **Memoria a corto plazo** (LSTM) y **Unidad de retransmisi√≥n cerrada** (GRU).

![Image showing an example long short term memory cell](./images/long-short-term-memory-cell.svg)

> Fuente de la imagen por determinar

La red LSTM est√° organizada de manera similar a RNN, pero hay dos estados que se pasan de una capa a otra: el estado real C y el vector oculto H. En cada unidad, el vector oculto H<sub>i< /sub> est√° concatenado con la entrada X<sub>i</sub>, y controlan lo que sucede con el estado C a trav√©s de **puertas**. Cada puerta es una red neuronal con activaci√≥n sigmoidea (salida en el rango [0,1]), que puede considerarse como una m√°scara bit a bit cuando se multiplica por el vector de estado. Existen las siguientes puertas (de izquierda a derecha en la imagen de arriba):

* La **puerta del olvido** toma un vector oculto y determina qu√© componentes del vector C debemos olvidar y por cu√°les pasar.
* La **puerta de entrada** toma informaci√≥n de los vectores ocultos y de entrada y la inserta en el estado.
* La **puerta de salida** transforma el estado a trav√©s de una capa lineal con activaci√≥n *tanh*, luego selecciona algunos de sus componentes usando un vector oculto H<sub>i</sub> para producir un nuevo estado C<sub>i+ 1</sub>.

Los componentes del estado C pueden considerarse como algunas banderas que se pueden activar y desactivar. Por ejemplo, cuando encontramos un nombre *Alice* en la secuencia, podemos asumir que se refiere a un personaje femenino y levantar la bandera en el estado de que tenemos un sustantivo femenino en la oraci√≥n. Cuando encontremos m√°s frases *y Tom*, levantaremos la bandera de que tenemos un sustantivo plural. As√≠, manipulando el estado supuestamente podemos realizar un seguimiento de las propiedades gramaticales de las partes de la oraci√≥n.

> ‚úÖ Un excelente recurso para comprender los aspectos internos de LSTM es este excelente art√≠culo [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
>
> ## RNN bidireccionales y multicapa

Hemos discutido redes recurrentes que operan en una direcci√≥n, desde el principio de una secuencia hasta el final. Parece natural porque se parece a la forma en que leemos y escuchamos el habla. Sin embargo, dado que en muchos casos pr√°cticos tenemos acceso aleatorio a la secuencia de entrada, podr√≠a tener sentido ejecutar c√°lculos recurrentes en ambas direcciones. Estas redes se denominan RNN **bidireccionales**. Cuando se trata de una red bidireccional, necesitar√≠amos dos vectores de estado ocultos, uno para cada direcci√≥n.

Una red recurrente, ya sea unidireccional o bidireccional, captura ciertos patrones dentro de una secuencia y puede almacenarlos en un vector de estado o pasarlos a la salida. Al igual que con las redes convolucionales, podemos construir otra capa recurrente encima de la primera para capturar patrones de nivel superior y construir a partir de patrones de bajo nivel extra√≠dos por la primera capa. Esto nos lleva a la noci√≥n de un **RNN multicapa** que consta de dos o m√°s redes recurrentes, donde la salida de la capa anterior se pasa a la siguiente capa como entrada.

![Image showing a Multilayer long-short-term-memory- RNN](./images/multi-layer-lstm.jpg)

*Photo de [this wonderful post](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) by Fernando L√≥pez*

## ‚úçÔ∏è Ejercicios: Incrustaciones

Contin√∫a tu aprendizaje en los siguientes cuadernos:

* [RNNs with PyTorch](RNNPyTorch.ipynb)
* [RNNs with TensorFlow](RNNTF.ipynb)

## Conclusi√≥n

En esta unidad, hemos visto que los RNN se pueden usar para la clasificaci√≥n de secuencias, pero de hecho, pueden manejar muchas m√°s tareas, como generaci√≥n de texto, traducci√≥n autom√°tica y m√°s. Consideraremos esas tareas en la siguiente unidad.

## üöÄ Desaf√≠o

Lea algo de literatura sobre LSTM y considere sus aplicaciones:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## Revisi√≥n y autoestudio

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah.

## [Assignment: Notebooks](assignment.md)




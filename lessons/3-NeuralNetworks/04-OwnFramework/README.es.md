# Introducci√≥n a las Redes Neuronales. Perceptr√≥n multicapa

En la secci√≥n anterior, aprendi√≥ sobre el modelo de red neuronal m√°s simple: el perceptr√≥n de una capa, un modelo de clasificaci√≥n lineal de dos clases.

En esta secci√≥n ampliaremos este modelo a un marco m√°s flexible, lo que nos permitir√°:

* realizar **clasificaci√≥n multiclase** adem√°s de dos clases
* resolver **problemas de regresi√≥n** adem√°s de clasificaci√≥n
* clases separadas que no son linealmente separables

Tambi√©n desarrollaremos nuestro propio framework modular en Python que nos permitir√° construir diferentes arquitecturas de redes neuronales.

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Formalizaci√≥n del aprendizaje autom√°tico

Comencemos por formalizar el problema del aprendizaje autom√°tico. Supongamos que tenemos un conjunto de datos de entrenamiento **X** con etiquetas **Y** y necesitamos crear un modelo *f* que haga las predicciones m√°s precisas. La calidad de las predicciones se mide mediante la **Funci√≥n de p√©rdida** &lagran;. A menudo se utilizan las siguientes funciones de p√©rdida:

* Para problemas de regresi√≥n, cuando necesitamos predecir un n√∫mero, podemos usar **error absoluto** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y< sup>(i)</sup>|, o **error al cuadrado** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i )</sup>)<sup>2</sup>
* Para la clasificaci√≥n, utilizamos **p√©rdida 0-1** (que es esencialmente lo mismo que **precisi√≥n** del modelo) o **p√©rdida log√≠stica**.

Para el perceptr√≥n de un nivel, la funci√≥n *f* se defini√≥ como una funci√≥n lineal *f(x)=wx+b* (aqu√≠ *w* es la matriz de peso, *x* es el vector de caracter√≠sticas de entrada y *b* es el vector de sesgo). Para diferentes arquitecturas de redes neuronales, esta funci√≥n puede adoptar una forma m√°s compleja.

> En el caso de la clasificaci√≥n, a menudo es deseable obtener probabilidades de las clases correspondientes como salida de la red. Para convertir n√∫meros arbitrarios en probabilidades (por ejemplo, para normalizar la salida), a menudo usamos la funci√≥n **softmax** Œ≥, y la funci√≥n *f* se convierte en *f(x)=Œ≥sigma;(wx+b)*

En la definici√≥n de *f* anterior, *w* y *b* se denominan **par√°metros** Œ∏=‚ü®*w,b*‚ü©. Dado el conjunto de datos ‚ü®**X**,**Y**‚ü©, podemos calcular un error general en todo el conjunto de datos en funci√≥n de los par√°metros Œ∏.

> ‚úÖ **El objetivo del entrenamiento de redes neuronales es minimizar el error variando los par√°metros Œ∏**

## Optimizaci√≥n del descenso de gradiente

Existe un m√©todo bien conocido de optimizaci√≥n de funciones llamado **descenso de gradiente**. La idea es que podemos calcular una derivada (en el caso multidimensional llamada **gradiente**) de la funci√≥n de p√©rdida con respecto a los par√°metros y variar los par√°metros de tal manera que el error disminuya. Esto se puede formalizar de la siguiente manera:

* Inicializa los par√°metros con algunos valores aleatorios w<sup>(0)</sup>, b<sup>(0)</sup>
*Repetir el siguiente paso muchas veces:
     - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
     - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

Durante el entrenamiento, se supone que los pasos de optimizaci√≥n se calculan considerando todo el conjunto de datos (recuerde que la p√©rdida se calcula como una suma de todas las muestras de entrenamiento). Sin embargo, en la vida real tomamos peque√±as porciones del conjunto de datos llamados **minilotes** y calculamos gradientes en funci√≥n de un subconjunto de datos. Debido a que el subconjunto se toma aleatoriamente cada vez, dicho m√©todo se denomina **descenso de gradiente estoc√°stico** (SGD).

## Perceptrones multicapa y retropropagaci√≥n

La red de una capa, como hemos visto anteriormente, es capaz de clasificar clases linealmente separables. Para construir un modelo m√°s rico, podemos combinar varias capas de la red. Matem√°ticamente significar√≠a que la funci√≥n *f* tendr√≠a una forma m√°s compleja y se calcular√≠a en varios pasos:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

Aqu√≠, &alfa; es una **funci√≥n de activaci√≥n no lineal**, &sigma; es una funci√≥n softmax y los par√°metros &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub >*>.

El algoritmo de descenso de gradientes seguir√≠a siendo el mismo, pero ser√≠a m√°s dif√≠cil calcular los gradientes. Dada la regla de diferenciaci√≥n de cadenas, podemos calcular las derivadas como:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part; z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part; z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w< sub>1</sub>)

> ‚úÖ La regla de diferenciaci√≥n de cadenas se utiliza para calcular las derivadas de la funci√≥n de p√©rdida con respecto a los par√°metros.

Tenga en cuenta que la parte m√°s a la izquierda de todas esas expresiones es la misma y, por lo tanto, podemos calcular derivadas de manera efectiva comenzando desde la funci√≥n de p√©rdida y yendo "hacia atr√°s" a trav√©s del gr√°fico computacional. Por lo tanto, el m√©todo de entrenamiento de un perceptr√≥n de m√∫ltiples capas se llama **backpropagation** o 'backprop'.

<img alt="calcular gr√°fico" src="images/ComputeGraphGrad.png"/>

> TODO: cita de imagen

> ‚úÖ Cubriremos el backprop con mucho m√°s detalle en nuestro ejemplo de cuaderno.

## Conclusi√≥n

En esta lecci√≥n, hemos creado nuestra propia biblioteca de redes neuronales y la hemos utilizado para una tarea sencilla de clasificaci√≥n bidimensional.

## üöÄ Desaf√≠o

En el cuaderno adjunto, implementar√° su propio marco para construir y entrenar perceptrones de m√∫ltiples capas. Podr√°s ver en detalle c√≥mo funcionan las redes neuronales modernas.

Proceder al [OwnFramework](OwnFramework.ipynb) notebook and work through it.

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## Revisi√≥n y autoestudio

La retropropagaci√≥n es un algoritmo com√∫n utilizado en IA y ML que vale la pena estudiar [in more detail](https://wikipedia.org/wiki/Backpropagation)

## [Assignment](lab/README.md)

En esta pr√°ctica de laboratorio, se le pedir√° que utilice el marco que construy√≥ en esta lecci√≥n para resolver la clasificaci√≥n de d√≠gitos escritos a mano de MNIST.

* [Instructions](lab/README.md)
* [Notebook](lab/MyFW_MNIST.ipynb)

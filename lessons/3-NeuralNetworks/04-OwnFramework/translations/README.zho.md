# 神经网络简介。多层感知器

在上一节中，我们学习了最简单的神经网络模型--单层感知器，这是一种线性两类分类模型。

在本节中，我们将把这一模型扩展到一个更灵活的框架中，使我们能够

* 除两级分类外，还能进行**多级分类**。
* 除了分类，还能解决**回归问题
* 分离不可线性分离的类别

我们还将用 Python 开发自己的模块化框架，以便构建不同的神经网络架构。

## [课前测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## 机器学习的形式化

让我们从机器学习问题的形式化开始。假设我们有一个标有**Y**的训练数据集**X**，我们需要建立一个能做出最准确预测的模型*f*。预测的质量由**损失函数** &lagran;来衡量。通常会用到以下损失函数：

* 对于回归问题，当我们需要预测一个数字时，我们可以使用**绝对误差**。 &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, 或**平方误差** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* 对于分类，我们使用**0-1损失**（与模型的**准确率**基本相同）或**逻辑损失**。

对于单级感知器，函数 *f* 被定义为线性函数 *f(x)=wx+b*（此处 *w* 为权重矩阵，*x* 为输入特征向量，*b* 为偏置向量）。对于不同的神经网络架构，该函数的形式可能更为复杂。

> 在分类的情况下，通常希望得到相应类别的概率作为网络输出。为了将任意数字转换为概率（例如，对输出进行归一化处理），我们经常使用 **softmax**函数 &sigma;，函数 *f* 变为 *f(x)=&sigma;(wx+b)*

在上面*f*的定义中，*w*和*b*被称为**参数** &theta;=⟨*w,b*⟩。给定数据集⟨**X**,**Y**⟩，我们可以计算出整个数据集的总体误差作为参数 &theta; 的函数。

> ✅ **神经网络训练的目标是通过改变参数 &theta;** 使误差最小化。

## 梯度下降优化法

有一种众所周知的函数优化方法叫**梯度下降**。其原理是，我们可以计算损失函数相对于参数的导数（在多维情况下称为**梯度**），并通过改变参数来减少误差。具体方法如下

* 用一些随机值初始化参数 w<sup>(0)</sup>, b<sup>(0)</sup>
* 重复以下步骤多次：
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

在训练过程中，优化步骤的计算应该考虑整个数据集（请记住，损失的计算是所有训练样本的总和）。然而，在现实生活中，我们会从数据集中抽取一小部分数据，称为**小批量**，然后根据数据子集计算梯度。由于每次都是随机抽取子集，因此这种方法被称为随机梯度下降**（SGD）。

## 多层感知器和反向传播

如上所述，单层网络能够对线性可分离的类别进行分类。为了建立更丰富的模型，我们可以将多层网络结合起来。从数学上讲，这意味着函数 *f* 的形式将更加复杂，并将分几步进行计算：
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

这里，&alpha;是一个**非线性激活函数**，&sigma;是一个软最大函数，参数为 &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

梯度下降算法将保持不变，但计算梯度将更加困难。根据链微分法则，我们可以计算出导数：

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> ✅链微分法则用于计算损失函数相对于参数的导数。

请注意，所有这些表达式的最左侧部分都是相同的，因此我们可以有效地从损失函数开始计算导数，并在计算图中 "倒推"。因此，训练多层感知器的方法被称为**反向推导**，或 "反向推导"。

<img alt="compute graph" src="../images/ComputeGraphGrad.png"/>

> ✅我们将在笔记本范例中更详细地介绍 backprop。 

## 结论

在本课中，我们建立了自己的神经网络库，并用它完成了一个简单的二维分类任务。

## 挑战

在随附的笔记本中，你将实现自己的多层感知器构建和训练框架。您将能够详细了解现代神经网络是如何运行的。

进入 [OwnFramework](../OwnFramework.ipynb) 笔记本并完成它。

## [课后测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## 复习与自学

反向传播是人工智能和 ML 中常用的算法，值得 [详细] 学习(https://wikipedia.org/wiki/Backpropagation)

## [Assignment](../lab/README.md)

本实验要求您使用本课构建的框架来解决 MNIST 手写数字分类问题。

* [说明](../lab/README.md)
* [笔记](../lab/MyFW_MNIST.ipynb)

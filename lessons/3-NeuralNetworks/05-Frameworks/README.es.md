# Marcos de redes neuronales

Como ya hemos aprendido, para poder entrenar redes neuronales de manera eficiente necesitamos hacer dos cosas:

* Para operar con tensores, por ej. para multiplicar, sumar y calcular algunas funciones como sigmoide o softmax
* Para calcular los gradientes de todas las expresiones, para realizar la optimizaci√≥n del descenso de gradientes.

  ## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

Si bien la biblioteca `numpy` puede hacer la primera parte, necesitamos alg√∫n mecanismo para calcular los gradientes. En [our framework](../04-OwnFramework/OwnFramework.ipynb) que hemos desarrollado en la secci√≥n anterior, tuvimos que programar manualmente todas las funciones derivadas dentro del m√©todo "hacia atr√°s", que realiza retropropagaci√≥n. Idealmente, un marco deber√≠a darnos la oportunidad de calcular gradientes de *cualquier expresi√≥n* que podamos definir.

Otra cosa importante es poder realizar c√°lculos en GPU o cualquier otra unidad de c√°lculo especializada, como [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit).El entrenamiento de redes neuronales profundas requiere *muchos* c√°lculos, y poder paralelizar esos c√°lculos en las GPU es muy importante.

> ‚úÖ El t√©rmino 'paralelizar' significa distribuir los c√°lculos en m√∫ltiples dispositivos.

Actualmente, los dos marcos neuronales m√°s populares son: [TensorFlow](http://TensorFlow.org) y [PyTorch](https://pytorch.org/). Ambos proporcionan una API de bajo nivel para operar con tensores tanto en CPU como en GPU. Adem√°s de la API de bajo nivel, tambi√©n existe una API de nivel superior, llamada [Keras](https://keras.io/) y [PyTorch Lightning](https://pytorchlightning.ai/) correspondientemente.

API de bajo nivel | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------|-------------------------------------|--------------------------------
API de alto nivel| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

Las **API de bajo nivel** en ambos marcos le permiten crear los llamados **gr√°ficos computacionales**. Este gr√°fico define c√≥mo calcular la salida (normalmente la funci√≥n de p√©rdida) con par√°metros de entrada determinados y se puede enviar para su c√°lculo en la GPU, si est√° disponible. Hay funciones para diferenciar este gr√°fico computacional y calcular gradientes, que luego se pueden usar para optimizar los par√°metros del modelo.

Las **API de alto nivel** consideran las redes neuronales como una **secuencia de capas** y facilitan mucho la construcci√≥n de la mayor√≠a de las redes neuronales. Entrenar el modelo generalmente requiere preparar los datos y luego llamar a una funci√≥n "adecuada" para hacer el trabajo.

La API de alto nivel le permite construir redes neuronales t√≠picas muy r√°pidamente sin preocuparse por muchos detalles. Al mismo tiempo, las API de bajo nivel ofrecen mucho m√°s control sobre el proceso de entrenamiento y, por lo tanto, se utilizan mucho en la investigaci√≥n cuando se trata de nuevas arquitecturas de redes neuronales.

Tambi√©n es importante comprender que puede utilizar ambas API juntas, por ejemplo. puede desarrollar su propia arquitectura de capa de red utilizando API de bajo nivel y luego usarla dentro de la red m√°s grande construida y entrenada con API de alto nivel. O puede definir una red utilizando la API de alto nivel como una secuencia de capas y luego usar su propio bucle de entrenamiento de bajo nivel para realizar la optimizaci√≥n. Ambas API utilizan los mismos conceptos b√°sicos subyacentes y est√°n dise√±adas para funcionar bien juntas.

## Aprendiendo

En este curso, ofrecemos la mayor parte del contenido tanto para PyTorch como para TensorFlow. Puedes elegir tu marco preferido y solo revisar los cuadernos correspondientes. Si no est√° seguro de qu√© marco elegir, lea algunas discusiones en Internet sobre **PyTorch vs. TensorFlow**. Tambi√©n puede echar un vistazo a ambos marcos para comprender mejor.

Siempre que sea posible, utilizaremos API de alto nivel para simplificar. Sin embargo, creemos que es importante comprender c√≥mo funcionan las redes neuronales desde cero, por lo que al principio comenzamos trabajando con API y tensores de bajo nivel. Sin embargo, si desea comenzar r√°pidamente y no quiere perder mucho tiempo aprendiendo estos detalles, puede omitirlos e ir directamente a los cuadernos de API de alto nivel.

## ‚úçÔ∏è Ejercicios: Marcos

Contin√∫a tu aprendizaje en los siguientes cuadernos:

API de bajo nivel | [TensorFlow+Keras Notebook](IntroKerasTF.ipynb) | [PyTorch](IntroPyTorch.ipynb)
--------------|-------------------------------------|--------------------------------
API de alto nivel| [Keras](IntroKeras.ipynb) | *PyTorch Lightning*

Despu√©s de dominar los marcos, recapitulemos la noci√≥n de sobreajuste.

# Sobreajuste

El sobreajuste es un concepto extremadamente importante en el aprendizaje autom√°tico y es muy importante hacerlo bien.

Considere el siguiente problema de aproximar 5 puntos (representado por "x" en los gr√°ficos siguientes):

![linear](../images/overfit1.jpg) | ![overfit](../images/overfit2.jpg)
-------------------------|--------------------------
**Modelo lineal, 2 par√°metros** | **Modelo no lineal, 7 par√°metros**
Error de entrenamiento = 5,3 | Error de entrenamiento = 0
Error de validaci√≥n = 5.1 | Error de validaci√≥n = 20

* A la izquierda vemos una buena aproximaci√≥n en l√≠nea recta. Debido a que el n√∫mero de par√°metros es adecuado, el modelo capta correctamente la idea detr√°s de la distribuci√≥n de puntos.
* A la derecha, el modelo es demasiado poderoso. Debido a que solo tenemos 5 puntos y el modelo tiene 7 par√°metros, se puede ajustar de tal manera que pase por todos los puntos, lo que hace que el error de entrenamiento sea 0. Sin embargo, esto impide que el modelo comprenda el patr√≥n correcto detr√°s de los datos, por lo que el error de validaci√≥n es muy alto.

Es muy importante lograr un equilibrio correcto entre la riqueza del modelo (n√∫mero de par√°metros) y el n√∫mero de muestras de entrenamiento.

## ¬øPor qu√© ocurre el sobreajuste?

   * No hay suficientes datos de entrenamiento
   * Modelo demasiado potente
   * Demasiado ruido en los datos de entrada

## C√≥mo detectar el sobreajuste

Como puede ver en el gr√°fico anterior, el sobreajuste se puede detectar mediante un error de entrenamiento muy bajo y un error de validaci√≥n alto. Normalmente, durante el entrenamiento, veremos que tanto los errores de entrenamiento como los de validaci√≥n comienzan a disminuir y luego, en alg√∫n momento, el error de validaci√≥n puede dejar de disminuir y comenzar a aumentar. Esto ser√° una se√±al de sobreajuste y el indicador de que probablemente deber√≠amos dejar de entrenar en este punto (o al menos tomar una instant√°nea del modelo).

![overfitting](../images/Overfitting.png)

## C√≥mo prevenir el sobreajuste

Si puede ver que se produce un sobreajuste, puede realizar una de las siguientes acciones:

  * Aumentar la cantidad de datos de entrenamiento.
  * Disminuir la complejidad del modelo.
  * Usa algo [regularization technique](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), como [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), lo que consideraremos m√°s adelante.

## Compensaci√≥n entre sobreajuste y sesgo-varianza

El sobreajuste es en realidad un caso de un problema m√°s gen√©rico en estad√≠stica llamado [Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Si consideramos las posibles fuentes de error en nuestro modelo, podemos ver dos tipos de errores:

* **Los errores de sesgo** se deben a que nuestro algoritmo no puede capturar correctamente la relaci√≥n entre los datos de entrenamiento. Puede deberse al hecho de que nuestro modelo no es lo suficientemente potente (**underfitting**).
* **Errores de varianza**, que se deben a que el modelo aproxima el ruido en los datos de entrada en lugar de una relaci√≥n significativa (**sobreajuste**).

Durante el entrenamiento, el error de sesgo disminuye (a medida que nuestro modelo aprende a aproximar los datos) y el error de varianza aumenta. Es importante detener el entrenamiento, ya sea manualmente (cuando detectamos un sobreajuste) o autom√°ticamente (al introducir la regularizaci√≥n), para evitar el sobreajuste.

## Conclusi√≥n

En esta lecci√≥n, aprendi√≥ sobre las diferencias entre las distintas API para los dos marcos de IA m√°s populares, TensorFlow y PyTorch. Adem√°s, aprendiste sobre un tema muy importante, el sobreajuste.

## üöÄ Desaf√≠o

En los cuadernos adjuntos encontrar√° "tareas" en la parte inferior; Trabajar en los cuadernos y completar las tareas.

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## Revisi√≥n y autoestudio

Investiga un poco sobre los siguientes temas:

- TensorFlow
-PyTorch
- Sobreajuste

Preg√∫ntate a ti mismo las siguientes preguntas:

- ¬øCu√°l es la diferencia entre TensorFlow y PyTorch?
- ¬øCu√°l es la diferencia entre sobreajuste y desajuste?

  ## [Assignment](lab/README.md)

En esta pr√°ctica de laboratorio, se le pedir√° que resuelva dos problemas de clasificaci√≥n utilizando redes completamente conectadas de una o varias capas utilizando PyTorch o TensorFlow.

* [Instructions](lab/README.md)
* [Notebook](lab/LabFrameworks.ipynb)
